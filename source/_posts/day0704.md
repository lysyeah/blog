---
title: "day0704_ml"
date: '2022-07-04 17:40:00'
---


## 확률적 경사 하강법
- 점진적 학습 (step, 보폭)
- 학습률
- XGBoost, LightGBM, 딥러닝(이미지 분류, 자연어 처리, 옵티마이저)

### 
- 신경망 이미지 데이터, 자연어
- 자율주행 하루 데이터 1TB --> 학습
- 한꺼번에 다 모델을 학습 어려움
  + 샘플링, 배치, 에포크, 오차(=손실=loss)가 가장 작은 지점을 찾아야 함. 
- 결론적으로, 확률적 경사 하강법

### 손실함수
- 로지스틱 손실 함수



```python
import pandas as pd 
fish = pd.read_csv("https://bit.ly/fish_csv_data")
fish.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 159 entries, 0 to 158
    Data columns (total 6 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   Species   159 non-null    object 
     1   Weight    159 non-null    float64
     2   Length    159 non-null    float64
     3   Diagonal  159 non-null    float64
     4   Height    159 non-null    float64
     5   Width     159 non-null    float64
    dtypes: float64(5), object(1)
    memory usage: 7.6+ KB
    

- 입력 데이터와 타깃 데이터 분리


```python
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

fish_input.shape, fish_target.shape
```




    ((159, 5), (159,))



- 훈련 세트와 테스트 데이터 분리


```python
from sklearn.model_selection import train_test_split 
train_input, test_input, train_target, test_target = train_test_split(
    # input, target, 옵션... 
    fish_input, fish_target, random_state=42
)
```

- 훈련 세트와 테스트 세트의 특성 표준화
  + 무게, 길이, 대각선 길이, 높이, 너비 
- 표준화 처리 진행
  


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# train_scaled[:5]
```

## 모델링
- 확률적 경사 하강법 


```python
from sklearn.linear_model import SGDClassifier 
sc = SGDClassifier(loss = 'log', max_iter = 10, random_state=42)

sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.773109243697479
    0.775
    

    /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      ConvergenceWarning,
    

- partial_fit() 메서드 사용하면 추가 학습. 


```python
sc.partial_fit(train_scaled, train_target) 
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.8151260504201681
    0.85
    

## 에포크와 과대/과소적합
- 에포크 숫자가 적으면 --> 덜 학습
- early_stopping
  + 에포크 숫자를 1000, 손실 10, 9, 8, , 3
  + 3에 도달한 시점이 150 


```python
import numpy as np 
sc = SGDClassifier(loss='log', random_state = 42)
train_score = []
test_score = []

classes = np.unique(train_target)

# 300번 에포크 훈련을 반복
# 훈련 할 때마다, train_score, test_score 추가를 한다. 
for _ in range(0, 300):
  sc.partial_fit(train_scaled, train_target, classes = classes)
  train_score.append(sc.score(train_scaled, train_target))
  test_score.append(sc.score(test_scaled, test_target)) 

```

- 시각화 


```python
import matplotlib.pyplot as plt 
plt.plot(train_score)
plt.plot(test_score)
plt.legend(["train", "test"])
plt.show()
```


    
![png](output_17_0.png)
    


## Chapter 5 트리 알고리즘 ( 화이트 와인을 찾아랏 )

### 결정트리
- wine 데이터 가져오기


```python
import pandas as pd 
wine = pd.read_csv('https://bit.ly/wine_csv_data')
print(wine.head())
```

       alcohol  sugar    pH  class
    0      9.4    1.9  3.51    0.0
    1      9.8    2.6  3.20    0.0
    2      9.8    2.3  3.26    0.0
    3      9.8    1.9  3.16    0.0
    4      9.4    1.9  3.51    0.0
    

- 데이터 가공하기 


```python
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
```


```python
wine['class'].value_counts()
```




    1.0    4898
    0.0    1599
    Name: class, dtype: int64



- 훈련데이터 분리


```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size = 0.2, random_state=42
)

train_input.shape, test_input.shape, train_target.shape, test_target.shape
```




    ((5197, 3), (1300, 3), (5197,), (1300,))



- 표준화 처리


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

- 모델 만들기


```python
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt 
from sklearn.tree import plot_tree 
dt = DecisionTreeClassifier(max_depth = 8, random_state=42)
# 4에 있는 'dt =' 에 dt = DecisionTreeClassifier(criterion = 'entropy', max_depth = 8, random_state=42) 넣으면 엔트로피로 나온다.
# [15]번에.
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

plt.figure(figsize=(10, 7))
plot_tree(dt)
plt.show()
```

    0.9003271117952665
    0.8576923076923076
    


    
![png](output_28_1.png)
    


- 훈련 정확도는 99.6%
- 테스트 정확도는 85.9% 
--> 과대적합이 일어남

## 노드란 무엇인가?
- 0 이면 레드 와인 :1599 개
- 1 이면 화이트 와인 :4898 개


```python
plt.figure(figsize=(10,7))
plot_tree(dt,max_depth=1,filled=True,feature_names=['alcohol','sugar','pH'])
plt.show()
```


    
![png](output_31_0.png)
    


- 불순도 ( gini 불순도 : 0.5 )
 + 비율
 + 레드와인 : 화이트와인 = 5:5 (데이터 비율)
 + 밸류 값이 5대5 일때 지니 값이 가장 크다..(분류가 잘 됐다)
 + 불순도 0.5가 기준이다.
-  한 범주안에서 서로 다른 데이터가 얼마나 섞여 있는지 나타냄
 + 흰색과 검은색이 각각 50개 섞여있다. = 불순도 0.5
 + 흰색과 검은색 완전 100% 분리가됨
 + 흰색 노드 불순도 최소 = 0
 + 검은색 노드 불순도 최소 = 0

- 엔트로피 ( Entropy )
 + 불확실한 정도를 의미한다. 0~1 사이
 + 흰색과 검은색이 각각 50개씩 섞여있따.
   + 엔트로피 최대 = 1
 + 완전 분리됨 
   + 흰색 노드 엔트로피 최소 = 0
   + 검은색 노드 엔트로피 최소 = 0

불순도로 하든 엔트로피로 하든 결과값은 거의 동일하다.
위에 [모델 만들기]에서 비교해볼 수 있다.

### 특성 중요도
- 어떤 특성이 결정 트리 모델에 영향을 주었는가?


```python
print(dt.feature_importances_)
## sugar 값이 가장 높은 걸 보았을 때 당도가 가장 중요하다는 결론을 도출할 수 있다.
### 특성 중요도는!! 인과 관계와는 관련이 없다.
```

    [0.17976778 0.65284899 0.16738324]
    

## 현업에서의 적용
- 현업에서 DecisionTreeClassifier ( 1970 년대에 이 개념이 나왔다. )
- 그렇기 때문에 업그레이드를 한 논문을 쓰게 된다.
 + 랜덤포레스트, XGBoost, ,,,, etc = 엄ㅁㅁㅁㅁㅁㅁ청 많다 그래서 다 못한다.



## 검증 세트
- 훈련 세트와 테스트 세트로 나누었다.
- 훈련 : 교과서( 로 공부하는 것 훈련세트, 모의 평가 )
- 검증 : 강남대성 모의고사
- 테스트 : 6월, 9월 평가
- 실전 : 수능 


```python
import pandas as pd 
wine = pd.read_csv('https://bit.ly/wine_csv_data')
#print(wine.head())


data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

# 훈련 80%
# 테스트 20%
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size = 0.2, random_state=42
)


train_input.shape, test_input.shape, train_target.shape, test_target.shape    
```




    ((5197, 3), (1300, 3), (5197,), (1300,))




```python
# 훈련 80%
# 검증 20%
sub_input, val_input, sub_target, val_target = train_test_split(
  train_input, train_target, test_size= 0.2, random_state=42
)

sub_input.shape, val_input.shape, sub_target.shape, val_target.shape
```




    ((4157, 3), (1040, 3), (4157,), (1040,))



- 훈련 데이터 : train (x), sub_input, sub_target이 훈련데이터이다.
- 검증 데이터 : val_input, val_target
- 테스트 데이터 : teset_input, test_target

- 모형 만들기 


```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print("훈련성과:",dt.score(sub_input, sub_target))
print("검증성과:", dt.score(val_input,val_target))
print("마지막 검증:", dt.score(test_input, test_target))
```

    훈련성과: 0.9971133028626413
    검증성과: 0.864423076923077
    마지막 검증: 0.8569230769230769
    

## 교차 검증
- 데이터 셋을 반복 분할
- For loop
- 샘플링 편향적일 수 있음. 그래서
- 교차 검증을 한다고 해서 무조건 정확도가 올라가는 것이 아니다.
- 단지, 모형을 '안정적'으로 만들어줄 뿐이다.
 +  방지도 해준다.


```python
import numpy as np
from sklearn. model_selection import KFold

df = np.array([1,2,3,4,5,6,7,8,9,10])

df

# 데이터를 K 폴드로 나눈다.
folds = KFold(n_splits=5, shuffle=True)
for train_idx, valid_idx in folds.split(df):
  print(f'훈련데이터:{df[train_idx]}, 검증데이터 : {df[valid_idx]}')
```

    훈련데이터:[ 1  2  3  4  6  8  9 10], 검증데이터 : [5 7]
    훈련데이터:[ 2  3  4  5  6  7  8 10], 검증데이터 : [1 9]
    훈련데이터:[ 1  2  3  5  7  8  9 10], 검증데이터 : [4 6]
    훈련데이터:[1 2 4 5 6 7 8 9], 검증데이터 : [ 3 10]
    훈련데이터:[ 1  3  4  5  6  7  9 10], 검증데이터 : [2 8]
    

- 교차 검증 함수


```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
print("평균:", np.mean(scores['test_score']))
```

    {'fit_time': array([0.0087018 , 0.00587964, 0.00602317, 0.00796366, 0.00969625]), 'score_time': array([0.00056958, 0.0004859 , 0.00047922, 0.00093341, 0.00075316]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
    평균: 0.855300214703487
    

- StratifiedKFold사


```python
from sklearn.model_selection import StratifiedKFold
splitter = StratifiedKFold(n_splits = 10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=StratifiedKFold())
print(scores)
print("평균:", np.mean(scores['test_score']))
```

    {'fit_time': array([0.02411795, 0.01521921, 0.01986766, 0.02518582, 0.01562715]), 'score_time': array([0.00103855, 0.00100446, 0.00081253, 0.00088406, 0.0007534 ]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
    평균: 0.855300214703487
    

## 하이퍼 파라미터 튜닝

- 그리드 서치
 + 사람이 수동적으로 입력하는 것.
 + max_depth로 할 때 : [1,3,7]

- 랜덤 서치
 + 사람이 범위만 정해준다.
 + amx_depth : 1 ~ 10 사이 / by random

- 베이지안 옵티마이제이션

- AutoML = 사람의 개입 없이 하이퍼파라미터 튜닝을 자동으로 수행하는 기술
 + 예) Pycaret, 

- 각 모델마다 적게는 1,2개 이고 많게는 5,6개의 매개변수를 제공.
 + XGBoost 100개...??????!?!?!?!?!??!

- 하이퍼파라미터와 동시에 교차검증을 수행
 + 미친짓..!!!!


- 교차검증 5번
 교차 검증 1번 돌 때, Max Depth 3번 적용
 + 총 결괏값 = 3 x 5 x 2 
 + Max Dept = 1, 3, 7
 + Criterion = gini, entropy 



```python
from sklearn.model_selection import GridSearchCV
params = {
    'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0004],
    'max_depth': [1,3,7],
    'criterion': ['gini','entropy']

}

gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params,n_jobs=-1)
gs.fit(train_input, train_target)

```




    GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
                 param_grid={'criterion': ['gini', 'entropy'],
                             'max_depth': [1, 3, 7],
                             'min_impurity_decrease': [0.0001, 0.0002, 0.0003,
                                                       0.0004, 0.0004]})




```python
print("best:",gs.best_estimator_)
dt = gs.best_estimator_
```

    best: DecisionTreeClassifier(criterion='entropy', max_depth=7,
                           min_impurity_decrease=0.0004, random_state=42)
    
