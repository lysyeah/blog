---
title: "day0705_ch5_ch6"
date: '2022-07-05 16:00:00'
---



## 트리의 앙상블
- 랜덤 포레스트 ( 나무1개 )
 + 여러 개 심음
 + 샘플링
 + Feature Importance
- 예측해야할 행의 갯수, 100만개
- 컬럼의 갯수 200개 ---> 100개
 + 나무 100개르 심고 평균을 내자
 + 나무 1개 당 컬럼을 10개로
 + T1 mae : 20 / T2 mae : 30 / T3 : mae : 10 ....
   + T1 ~ T100 mae : 200
   + Feature Importance
 + sampling : bootstrap  (복원추출)




```python
# 라이브러리 불러오기
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# 데이터 불러오기
wine = pd.read_csv('http://bit.ly/wine_csv_data')

# input, target 분리
data = wine [['alcohol','sugar','pH']].to_numpy()
target = wine['class'].to_numpy()

# 훈련 데이터, 테스트 데이터 분리
train_input, test_input, train_target, test_target = train_test_split(
  data, target, test_size=0.2, random_state=42)

# 모델링
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)

# 모형 평가
scores = cross_validate(rf,train_input,train_target,
                        return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))

# 특성 중요
rf.fit(train_input, train_target)
print(rf.feature_importances_)

```

    0.9973541965122431 0.8905151032797809
    [0.23167441 0.50039841 0.26792718]
    


```python
# 00B
rf = RandomForestClassifier(oob_score = True, n_jobs=-1,random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)

```

    0.8934000384837406
    

- 엑스트라 트리


```python
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.9974503966084433 0.8887848893166506
    


```python
et.fit(train_input, train_target)
print(et.feature_importances_)
```

    [0.20183568 0.52242907 0.27573525]
    

- 그레이디언트 부스팅 # 개념만 이해하면 된다.

  + 경사하강법의 원리를 이용함
  + T1 ~ TN 증가하면서 오차를 보정해주며 정확성을  높임
  + 랜덤포레스트와의 차이점
    + 랜덤포레스트는 각 나무 간 상호 연관성 없음
    + 부스팅은 각 나무 간 상호 연관성 있음
  + 단점 
    + 속도가 너무 느리다
  + 대안
    + XBoost, LightGBM


```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.8881086892152563 0.8720430147331015
    


```python
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.9464595437171814 0.8780082549788999
    


```python
gb.fit(train_input, train_target)
print(gb.feature_importances_)
```

    [0.15872278 0.68010884 0.16116839]
    

## 히스토그램 기반 그레이디언트 부스팅 
- 하지말기.

## 06-1 군집 알고리즘 
### 실무에서의 난이도
비지도학습 >>> 지도 학습
비지도 학습
--> 분류
--> 수치적으로 분류

주성분 분석
- 이론적으로는 어려움
- 좌표계 공간 개념
- 직교 + 회전
- 공분산 등 ( 통계 관련 내용 )
- Feature Engineering 기법
- StandardScaler()
- 현 ML(Machine LEarning)의 문제점:컬럼의 갯수가 너무 !! 많다.
 + 왜 많냐면 .. 
- 차원 축소
 + 특성이 많으면 훈련 데이터에 쉽게 과대적합된다.
 + 특성을 줄여서 학습 모델의 성능을 향산시킨다.
 + 모델의 학습 시간을 감소시켜줌
 + 대표적인 방법론 중 둘 : PCA, EFA

- EFA / PCA
 + EFA( 탐색적 요인 분석 ) (= Factor Analysis )
    - 국어, 수학, 과학, 영어
    - 국어 40, 수학 100 과학 100 영어 30/
    귀 학생을 언어영억은 수준이 낮은편이나 수리영역은 매우 수준이 높습니다.
    - 범주형 & 수치 데이터 셋
 
 
 + PCA ( 주성분 분석 )
    - 장비1, 장비2, 장비3, , , , 
    - pc1, pc2, pc3, pc4,,,
    - 원래 가지고 있던 정보를 알 수 없음.(정보손실)
    - 범주형 데이터셋에는 사용 안됨
    - 무조건 수치형 데이터에서만 사용
    - pca 하기 전, 표준화하기(스케일링 실행)=
    x축, y축의 단위 동일화하기.



```python
# 데이터 가져오기 
!wget https://bit.ly/fruits_300_data -O fruits_300.npy # zero아니고 alphabet O다.
# p.287에 ! 는 뭔가요 보기.
```

    --2022-07-05 04:47:49--  https://bit.ly/fruits_300_data
    Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10
    Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following]
    --2022-07-05 04:47:49--  https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy
    Resolving github.com (github.com)... 192.30.255.112
    Connecting to github.com (github.com)|192.30.255.112|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following]
    --2022-07-05 04:47:50--  https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy
    Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 3000128 (2.9M) [application/octet-stream]
    Saving to: ‘fruits_300.npy’
    
    fruits_300.npy      100%[===================>]   2.86M  --.-KB/s    in 0.05s   
    
    2022-07-05 04:47:50 (60.3 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128]
    
    


```python
# 라이브러리 불러오기
import numpy as np
fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1,100*100)

# 확인이나 해보자
print(fruits)
print(fruits_2d)
```

    [[[1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      ...
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]]
    
     [[1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      ...
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]]
    
     [[1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      ...
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]]
    
     ...
    
     [[1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      ...
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]]
    
     [[1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      ...
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]]
    
     [[1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      ...
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]
      [1 1 1 ... 1 1 1]]]
    [[1 1 1 ... 1 1 1]
     [1 1 1 ... 1 1 1]
     [1 1 1 ... 1 1 1]
     ...
     [1 1 1 ... 1 1 1]
     [1 1 1 ... 1 1 1]
     [1 1 1 ... 1 1 1]]
    


```python
# 모델링
from sklearn.decomposition import PCA
pca = PCA(n_components = 50 )
pca.fit(fruits_2d)
# 10,000개의 데이터를 50개로 줄인다는 뜻이다.
```




    PCA(n_components=50)




```python
print(pca.components_.shape) 
```

    (50, 10000)
    


```python
import matplotlib.pyplot as plt

def draw_fruits(arr, ratio=1):
    n = len(arr)    # n은 샘플 개수입니다
    # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. 
    rows = int(np.ceil(n/10))
    # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다.
    cols = n if rows < 2 else 10
    fig, axs = plt.subplots(rows, cols, 
                            figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n:    # n 개까지만 그립니다.
                axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()
```


```python
print(pca.components_.reshape(-1,100,100))
# 나는 왜 그림이 나오지 않는 것인가 !! 
```

    [[[ 9.24490195e-06  5.10601298e-06  8.91640398e-06 ...  3.64260921e-06
        9.34252918e-07  4.67126459e-06]
      [ 1.15833514e-05  5.76383497e-07  4.14828737e-06 ...  0.00000000e+00
        9.02785432e-07  0.00000000e+00]
      [ 5.83134163e-06  2.29451889e-06  1.61700364e-06 ...  0.00000000e+00
        4.60832962e-06  0.00000000e+00]
      ...
      [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
        0.00000000e+00  0.00000000e+00]
      [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
        0.00000000e+00  0.00000000e+00]
      [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
        0.00000000e+00  0.00000000e+00]]
    
     [[-1.69701613e-05 -1.13551047e-05 -1.82152561e-05 ... -6.85415826e-06
       -1.72461747e-06 -8.62308735e-06]
      [-1.83988224e-05 -5.12361973e-06 -6.86007645e-06 ... -0.00000000e+00
       -1.70984693e-06 -0.00000000e+00]
      [-9.48385290e-06 -3.28956277e-06 -1.59769056e-06 ... -0.00000000e+00
       -8.59354627e-06 -0.00000000e+00]
      ...
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]]
    
     [[-3.65220479e-05 -2.08401744e-05 -2.89870138e-05 ... -1.07455043e-05
       -2.28430243e-06 -1.14215121e-05]
      [-4.71494348e-05 -3.76266648e-06 -1.07501288e-05 ... -0.00000000e+00
       -2.82040061e-06 -0.00000000e+00]
      [-2.23009741e-05 -8.26967039e-06 -6.95529298e-06 ... -0.00000000e+00
       -1.24937085e-05 -0.00000000e+00]
      ...
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]]
    
     ...
    
     [[-2.51432593e-05 -7.92600954e-05 -8.23607786e-05 ... -1.83469572e-06
       -4.11988751e-06 -2.05994376e-05]
      [-7.92507818e-05 -2.81531763e-05 -3.39914008e-05 ... -0.00000000e+00
        7.61730597e-07 -0.00000000e+00]
      [-5.33663918e-05 -4.56116856e-05 -3.15206037e-05 ... -0.00000000e+00
       -1.08362013e-05 -0.00000000e+00]
      ...
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]]
    
     [[ 2.57743887e-05  7.93202087e-05  1.00202288e-04 ... -2.14844721e-05
       -2.89642108e-06 -1.44821054e-05]
      [ 2.31050308e-06  6.06796251e-05  3.07111026e-05 ... -0.00000000e+00
       -6.19601701e-06 -0.00000000e+00]
      [ 6.96702046e-05  2.55759163e-05 -5.23724211e-06 ... -0.00000000e+00
       -2.10812973e-05 -0.00000000e+00]
      ...
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]
      [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00
       -0.00000000e+00 -0.00000000e+00]]
    
     [[ 1.37760143e-04 -5.05654438e-05 -1.52581078e-05 ...  9.64447604e-05
        1.79898700e-05  8.99493501e-05]
      [ 1.00495763e-04 -7.18134056e-06 -1.82977600e-05 ...  0.00000000e+00
        2.61516301e-05  0.00000000e+00]
      [-6.08594244e-05 -6.22125611e-05  1.39415799e-05 ...  0.00000000e+00
        1.06272870e-04  0.00000000e+00]
      ...
      [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
        0.00000000e+00  0.00000000e+00]
      [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
        0.00000000e+00  0.00000000e+00]
      [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
        0.00000000e+00  0.00000000e+00]]]
    


```python
print(fruits_2d.shape)
```

    (300, 10000)
    


```python
# ML 에서 컬럼의 갯수를 10,000개를 50개로 줄인 것인다.. ( =압축 )
fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)
```

    (300, 50)
    

- 원본 데이터 재구성 


```python
fruits_inverse = pca.inverse_transform(fruits_pca)
print(fruits_inverse.shape)
```

    (300, 10000)
    


```python
fruits_reconstruct = fruits_inverse.reshape(-1,100,100)
for start in [0,100,200]:
  draw_fruits(fruits_reconstruct[start:start+100])
  print("\n")
```


    
![png](output_22_0.png)
    


    
    
    


    
![png](output_22_2.png)
    


    
    
    


    
![png](output_22_4.png)
    


    
    
    

- 설명된 분산
 + = 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값


```python
# 92%
# 원본 이미지 압축

print(np.sum(pca.explained_variance_ratio_))
```

    0.9215747829079223
    


```python
plt.plot(pca.explained_variance_ratio_)
plt.show()
```


    
![png](output_25_0.png)
    


# PCA의 기능을 알아두자.
##### 정확도는 조금 떨어지지만(92%) 속도가 빨라진다 (20배) 


```python


```
